{"method": "GET", "url": "http://ip-00001:9100/api/versioned/v1/operations/818", "status": 200, "content_type": "application/json", "body": "{\"id\":\"818\",\"type\":\"SPARK\",\"description\":\"Intentionally failed job\",\"status\":{\"state\":\"FAILED\",\"startTime\":\"2021-01-29T18:24:53.326Z\",\"endTime\":\"2021-01-29T18:25:17.300Z\",\"message\":\"org.apache.spark.sql.AnalysisException: cannot resolve '`sourceId`' given input columns: [nickname, id, first_name, ssn, last_name];;\\n'Project [to_string('sourceId) AS sourceId#15, to_string('entityId) AS entityId#16, to_string('originSourceId) AS originSourceId#17, to_string('originEntityId) AS originEntityId#18, to_string('city) AS city#19, to_string('birth_date) AS birth_date#20, to_string('data_source) AS data_source#21, to_string('lastname) AS lastname#22, to_string('address) AS address#23, to_string('country) AS country#24, to_string('fullname) AS fullname#25, to_string('data_sub_source) AS data_sub_source#26, to_string('firstname) AS firstname#27, to_string('gender) AS gender#28, to_string('guid) AS guid#29, to_string('marital_status) AS marital_status#30, to_string('zipcode5) AS zipcode5#31, to_string('zipcode4) AS zipcode4#32, to_string('title) AS title#33, to_string('state_code) AS state_code#34, to_string('phone) AS phone#35, to_string('middlename) AS middlename#36, to_string('source_guid) AS source_guid#37, to_string('birth_date_diff) AS birth_date_diff#38, ... 6 more fields]\\n+- LogicalRDD [id#5, first_name#6, nickname#7, last_name#8, ssn#9], false\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\\n\\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\\n\\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:122)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:115)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:108)\\n\\tat com.tamr.transform.TransformFactory$8.apply(TransformFactory.java:188)\\n\\tat com.tamr.transform.TransformsEvaluator.eval(TransformsEvaluator.java:36)\\n\\tat com.tamr.transform.TransformsEvaluator.eval(TransformsEvaluator.java:48)\\n\\tat com.tamr.transform.spark.AbstractTransformComponent.process(AbstractTransformComponent.java:65)\\n\\tat com.tamr.workflow.spark.DagRunner.invokeComponent(DagRunner.java:93)\\n\\tat com.tamr.workflow.spark.DagRunner.lambda$accept$0(DagRunner.java:53)\\n\\tat com.tamr.collections.Dag.reduce(Dag.java:404)\\n\\tat com.tamr.workflow.spark.DagRunner.accept(DagRunner.java:56)\\n\\tat com.tamr.workflow.spark.DagRunner.accept(DagRunner.java:26)\\n\\tat com.tamr.pipeline.driver.PipelineDriver.run(PipelineDriver.java:97)\\n\\tat com.tamr.pipeline.driver.PipelineDriver.main(PipelineDriver.java:178)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:685)\\n\"},\"created\":{\"username\":\"admin\",\"time\":\"2021-01-29T18:24:27.811Z\",\"version\":\"71389\"},\"lastModified\":{\"username\":\"system\",\"time\":\"2021-01-29T18:25:21.931Z\",\"version\":\"71403\"},\"relativeId\":\"operations/818\"}"}
{"method": "GET", "url": "http://ip-00001:9100/api/versioned/v1/operations/818", "status": 200, "content_type": "application/json", "body": "{\"id\":\"818\",\"type\":\"SPARK\",\"description\":\"Intentionally failed job\",\"status\":{\"state\":\"FAILED\",\"startTime\":\"2021-01-29T18:24:53.326Z\",\"endTime\":\"2021-01-29T18:25:17.300Z\",\"message\":\"org.apache.spark.sql.AnalysisException: cannot resolve '`sourceId`' given input columns: [nickname, id, first_name, ssn, last_name];;\\n'Project [to_string('sourceId) AS sourceId#15, to_string('entityId) AS entityId#16, to_string('originSourceId) AS originSourceId#17, to_string('originEntityId) AS originEntityId#18, to_string('city) AS city#19, to_string('birth_date) AS birth_date#20, to_string('data_source) AS data_source#21, to_string('lastname) AS lastname#22, to_string('address) AS address#23, to_string('country) AS country#24, to_string('fullname) AS fullname#25, to_string('data_sub_source) AS data_sub_source#26, to_string('firstname) AS firstname#27, to_string('gender) AS gender#28, to_string('guid) AS guid#29, to_string('marital_status) AS marital_status#30, to_string('zipcode5) AS zipcode5#31, to_string('zipcode4) AS zipcode4#32, to_string('title) AS title#33, to_string('state_code) AS state_code#34, to_string('phone) AS phone#35, to_string('middlename) AS middlename#36, to_string('source_guid) AS source_guid#37, to_string('birth_date_diff) AS birth_date_diff#38, ... 6 more fields]\\n+- LogicalRDD [id#5, first_name#6, nickname#7, last_name#8, ssn#9], false\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\\n\\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\\n\\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:122)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:115)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:108)\\n\\tat com.tamr.transform.TransformFactory$8.apply(TransformFactory.java:188)\\n\\tat com.tamr.transform.TransformsEvaluator.eval(TransformsEvaluator.java:36)\\n\\tat com.tamr.transform.TransformsEvaluator.eval(TransformsEvaluator.java:48)\\n\\tat com.tamr.transform.spark.AbstractTransformComponent.process(AbstractTransformComponent.java:65)\\n\\tat com.tamr.workflow.spark.DagRunner.invokeComponent(DagRunner.java:93)\\n\\tat com.tamr.workflow.spark.DagRunner.lambda$accept$0(DagRunner.java:53)\\n\\tat com.tamr.collections.Dag.reduce(Dag.java:404)\\n\\tat com.tamr.workflow.spark.DagRunner.accept(DagRunner.java:56)\\n\\tat com.tamr.workflow.spark.DagRunner.accept(DagRunner.java:26)\\n\\tat com.tamr.pipeline.driver.PipelineDriver.run(PipelineDriver.java:97)\\n\\tat com.tamr.pipeline.driver.PipelineDriver.main(PipelineDriver.java:178)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:685)\\n\"},\"created\":{\"username\":\"admin\",\"time\":\"2021-01-29T18:24:27.811Z\",\"version\":\"71389\"},\"lastModified\":{\"username\":\"system\",\"time\":\"2021-01-29T18:25:21.931Z\",\"version\":\"71403\"},\"relativeId\":\"operations/818\"}"}
{"method": "GET", "url": "http://ip-00001:9100/api/versioned/v1/operations/818", "status": 200, "content_type": "application/json", "body": "{\"id\":\"818\",\"type\":\"SPARK\",\"description\":\"Intentionally failed job\",\"status\":{\"state\":\"FAILED\",\"startTime\":\"2021-01-29T18:24:53.326Z\",\"endTime\":\"2021-01-29T18:25:17.300Z\",\"message\":\"org.apache.spark.sql.AnalysisException: cannot resolve '`sourceId`' given input columns: [nickname, id, first_name, ssn, last_name];;\\n'Project [to_string('sourceId) AS sourceId#15, to_string('entityId) AS entityId#16, to_string('originSourceId) AS originSourceId#17, to_string('originEntityId) AS originEntityId#18, to_string('city) AS city#19, to_string('birth_date) AS birth_date#20, to_string('data_source) AS data_source#21, to_string('lastname) AS lastname#22, to_string('address) AS address#23, to_string('country) AS country#24, to_string('fullname) AS fullname#25, to_string('data_sub_source) AS data_sub_source#26, to_string('firstname) AS firstname#27, to_string('gender) AS gender#28, to_string('guid) AS guid#29, to_string('marital_status) AS marital_status#30, to_string('zipcode5) AS zipcode5#31, to_string('zipcode4) AS zipcode4#32, to_string('title) AS title#33, to_string('state_code) AS state_code#34, to_string('phone) AS phone#35, to_string('middlename) AS middlename#36, to_string('source_guid) AS source_guid#37, to_string('birth_date_diff) AS birth_date_diff#38, ... 6 more fields]\\n+- LogicalRDD [id#5, first_name#6, nickname#7, last_name#8, ssn#9], false\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\\n\\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\\n\\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:122)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:115)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:108)\\n\\tat com.tamr.transform.TransformFactory$8.apply(TransformFactory.java:188)\\n\\tat com.tamr.transform.TransformsEvaluator.eval(TransformsEvaluator.java:36)\\n\\tat com.tamr.transform.TransformsEvaluator.eval(TransformsEvaluator.java:48)\\n\\tat com.tamr.transform.spark.AbstractTransformComponent.process(AbstractTransformComponent.java:65)\\n\\tat com.tamr.workflow.spark.DagRunner.invokeComponent(DagRunner.java:93)\\n\\tat com.tamr.workflow.spark.DagRunner.lambda$accept$0(DagRunner.java:53)\\n\\tat com.tamr.collections.Dag.reduce(Dag.java:404)\\n\\tat com.tamr.workflow.spark.DagRunner.accept(DagRunner.java:56)\\n\\tat com.tamr.workflow.spark.DagRunner.accept(DagRunner.java:26)\\n\\tat com.tamr.pipeline.driver.PipelineDriver.run(PipelineDriver.java:97)\\n\\tat com.tamr.pipeline.driver.PipelineDriver.main(PipelineDriver.java:178)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:685)\\n\"},\"created\":{\"username\":\"admin\",\"time\":\"2021-01-29T18:24:27.811Z\",\"version\":\"71389\"},\"lastModified\":{\"username\":\"system\",\"time\":\"2021-01-29T18:25:21.931Z\",\"version\":\"71403\"},\"relativeId\":\"operations/818\"}"}
{"method": "GET", "url": "http://ip-00001:9100/api/versioned/v1/operations/818", "status": 200, "content_type": "application/json", "body": "{\"id\":\"818\",\"type\":\"SPARK\",\"description\":\"Intentionally failed job\",\"status\":{\"state\":\"FAILED\",\"startTime\":\"2021-01-29T18:24:53.326Z\",\"endTime\":\"2021-01-29T18:25:17.300Z\",\"message\":\"org.apache.spark.sql.AnalysisException: cannot resolve '`sourceId`' given input columns: [nickname, id, first_name, ssn, last_name];;\\n'Project [to_string('sourceId) AS sourceId#15, to_string('entityId) AS entityId#16, to_string('originSourceId) AS originSourceId#17, to_string('originEntityId) AS originEntityId#18, to_string('city) AS city#19, to_string('birth_date) AS birth_date#20, to_string('data_source) AS data_source#21, to_string('lastname) AS lastname#22, to_string('address) AS address#23, to_string('country) AS country#24, to_string('fullname) AS fullname#25, to_string('data_sub_source) AS data_sub_source#26, to_string('firstname) AS firstname#27, to_string('gender) AS gender#28, to_string('guid) AS guid#29, to_string('marital_status) AS marital_status#30, to_string('zipcode5) AS zipcode5#31, to_string('zipcode4) AS zipcode4#32, to_string('title) AS title#33, to_string('state_code) AS state_code#34, to_string('phone) AS phone#35, to_string('middlename) AS middlename#36, to_string('source_guid) AS source_guid#37, to_string('birth_date_diff) AS birth_date_diff#38, ... 6 more fields]\\n+- LogicalRDD [id#5, first_name#6, nickname#7, last_name#8, ssn#9], false\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\\n\\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\\n\\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:122)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:115)\\n\\tat com.tamr.transform.parser.SparkTransformsEvaluator.select(SparkTransformsEvaluator.java:108)\\n\\tat com.tamr.transform.TransformFactory$8.apply(TransformFactory.java:188)\\n\\tat com.tamr.transform.TransformsEvaluator.eval(TransformsEvaluator.java:36)\\n\\tat com.tamr.transform.TransformsEvaluator.eval(TransformsEvaluator.java:48)\\n\\tat com.tamr.transform.spark.AbstractTransformComponent.process(AbstractTransformComponent.java:65)\\n\\tat com.tamr.workflow.spark.DagRunner.invokeComponent(DagRunner.java:93)\\n\\tat com.tamr.workflow.spark.DagRunner.lambda$accept$0(DagRunner.java:53)\\n\\tat com.tamr.collections.Dag.reduce(Dag.java:404)\\n\\tat com.tamr.workflow.spark.DagRunner.accept(DagRunner.java:56)\\n\\tat com.tamr.workflow.spark.DagRunner.accept(DagRunner.java:26)\\n\\tat com.tamr.pipeline.driver.PipelineDriver.run(PipelineDriver.java:97)\\n\\tat com.tamr.pipeline.driver.PipelineDriver.main(PipelineDriver.java:178)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:685)\\n\"},\"created\":{\"username\":\"admin\",\"time\":\"2021-01-29T18:24:27.811Z\",\"version\":\"71389\"},\"lastModified\":{\"username\":\"system\",\"time\":\"2021-01-29T18:25:21.931Z\",\"version\":\"71403\"},\"relativeId\":\"operations/818\"}"}
